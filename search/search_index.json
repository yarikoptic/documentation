{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Neurobagel","text":""},{"location":"#preparing-the-data","title":"Preparing the data","text":"<p>To use the Neurobagel Annotation tool,  please prepare your tabular data as a single file.</p> <p>Here are some examples:</p>"},{"location":"#a-bids-participantstsv-file","title":"A BIDS participants.tsv file","text":"<p>If you have a BIDS compliant participants.tsv file that contains  all of your demographic information,  then you can annotate this file with the Neurobagel Annotator to create a new data dictionary. </p> <p>For example:</p> participant_id age sex tools sub-01 22 female WASI-2 sub-02 28 male Stroop ..."},{"location":"#a-multi-session-file","title":"A multi-session file","text":"<p>If you have multi-session tabular data (e.g. different ages for different sessions), then you should combine all information into a single tabular file.</p> <p>Note</p> <p>A participants.tsv file with multiple sessions is not BIDS compliant. If you want to store a multi-session file in a BIDS dataset, you could do so in the <code>/pheno</code> subdirector.</p> <p>For example:</p> participant_id session_id age tools sub-01 ses-01 22 WASI-2 sub-01 ses-02 23 sub-02 ses-01 28 Stroop ..."},{"location":"#multiple-participant-or-session-ids","title":"Multiple participant or session IDs","text":"<p>In some cases there may be a need for more than one set of IDs  for participants and/or sessions. For example if a participant was first enrolled in a behavioural study with one type of IDs  and later joined an imaging study with different IDs. In such a case, you should include both participant IDs in the tabular file. The only requirement is that the combination of IDs has to be unique.</p> <p>For example, this would not be allowed:</p> participant_id alternative_participant_id ... sub-01 SID-1234 sub-01 SID-2222 sub-02 SID-1234 <p>The same rules apply to session IDs. </p> <p>For example:</p> participant_id alt_participant_id session_id alt_session_id age ... sub-01 SID-1234 ses-01 visit-1 22 sub-01 SID-1234 ses-02 visit-2 23 sub-02 SID-2222 ses-01 visit-1 28 ..."},{"location":"datamodels/","title":"Neurobagel Data Models","text":""},{"location":"datamodels/#data-dictionaries","title":"Data Dictionaries","text":"<p>When you annotate a demographic <code>.csv</code> file with the Neurobagel Annotation tool, your annotations are stored in a data dictionary. Neurobagel uses a structure for these data dictionaries that is compatible  with and expands on the  BIDS <code>participant.json</code> data dictionaries. </p> <p>Info</p> <p>The specification for how a Neurobagel data dictionary is structured is also called a schema.  Because Neurobagel data dictionaries are stored as <code>.json</code> files, we use the <code>jsonschema</code> schema language  to write the specification.</p> <p>The Neurobagel data dictionaries add a new <code>Annotations</code> attribute  to each column entry to store the semantic annotations.</p> <p>Here is an example of a BIDS data dictionary:</p> <pre><code>{\n\"age\": {\n\"Description\": \"age of the participant\",\n\"Units\": \"years\"\n},\n\"sex\": {\n\"Description\": \"sex of the participant as reported by the participant\",\n\"Levels\": {\n\"M\": \"male\",\n\"F\": \"female\"\n}\n}\n}\n</code></pre> <p>And here is the same example with Neurobagel annotations added:</p> <pre><code>{\n\"age\": {\n\"Description\": \"age of the participant\",\n\"Units\": \"years\",\n\"Annotations\": {\n\"IsAbout\": {\n\"TermURL\": \"http://neurobagel.org/vocab/Age\",\n\"Label\": \"Age\"\n},\n\"Transformation\": {\n\"TermURL\": \"http://neurobagel.org/vocab/int\",\n\"Label\": \"Integer\"\n}\n}\n},\n\"sex\": {\n\"Description\": \"sex of the participant as reported by the participant\",\n\"Levels\": {\n\"M\": \"male\",\n\"F\": \"female\"\n},\n\"Annotations\": {\n\"IsAbout\": {\n\"TermURL\": \"http://neurobagel.org/vocab/Sex\",\n\"Label\": \"Sex\"\n},\n\"Levels\": {\n\"M\": {\n\"TermURL\": \"http://purl.bioontology.org/ontology/SNOMEDCT/248153007\",\n\"Label\": \"Male\"\n},\n\"F\": {\n\"TermURL\": \"http://purl.bioontology.org/ontology/SNOMEDCT/248152002\",\n\"Label\": \"Female\"\n}\n},\n\"MissingValues\": [\n\"\",\n\" \"\n]\n}\n}\n}\n</code></pre> <p>Note that we use a Neurobagel namespace (URI: http://neurobagel.org/vocab/) for controlled terms representing classes or properties that we model, such as <code>\"Age\"</code> and <code>\"Sex\"</code>, but that these can have equivalent terms in another namespace we are using. For example, the following terms from the Neurobagel annotations above are conceptually equivalent to terms from the SNOMED-CT namespace:</p> Neurobagel namespace term External controlled vocabulary term http://neurobagel.org/vocab/Age http://purl.bioontology.org/ontology/SNOMEDCT/397669002 http://neurobagel.org/vocab/Sex http://purl.bioontology.org/ontology/SNOMEDCT/184100006"},{"location":"dictionaries/","title":"Neurobagel Data Dictionaries","text":"<p>The Neurobagel annotator creates a BIDS compatible data dictionary and augments the information that BIDS recommends with  unambiguous semantic tags.</p> <p>Below we'll outline several special cases using the following example <code>participants.tsv</code> file:</p> participant_id session_id group age sex updrs_1 updrs_2 sub-01 ses-01 PAT 25 M 2 sub-01 ses-02 PAT 26 M 3 5 sub-02 ses-01 CTL 28 F 1 1 sub-02 ses-02 CTL 29 F 1 1 <p>Controlled terms in the below examples are shortened using the RDF prefix/context syntax for json-ld:</p> <pre><code>{\n\"@context\": {\n\"nb\": \"http://neurobagel.org/vocab/\",\n\"purl\": \"http://purl.obolibrary.org/obo/\",\n\"snomed\": \"http://purl.bioontology.org/ontology/SNOMEDCT/\",\n\"cogatlas\": \"https://www.cognitiveatlas.org/task/id/\"\n}\n}\n</code></pre>"},{"location":"dictionaries/#participant-identifier","title":"Participant identifier","text":"<p>Term from the Neurobagel vocabulary.</p> <pre><code>{\n\"participant_id\": {\n\"Description\": \"A participant ID\",\n\"Annotations\": {\n\"IsAbout\": {\n\"TermURL\": \"nb:ParticipantID\",\n\"Label\": \"Subject Unique Identifier\"\n}\n}\n}\n}\n</code></pre> <p>Note</p> <p><code>participant_id</code> is a reserved name in BIDS and BIDS data dictionaries therefore typically don't annotate this column. Neurobagel supports multiple subject ID columns for situations where a study is using more than one ID scheme.</p>"},{"location":"dictionaries/#session-identifier","title":"Session identifier","text":"<p>Term from the Neurobagel vocabulary.</p> <pre><code>{\n\"session_id\": {\n\"Description\": \"A session ID\",\n\"Annotations\": {\n\"IsAbout\": {\n\"TermURL\": \"nb:SessionID\",\n\"Label\": \"Run Identifier\"\n}\n}\n}\n}\n</code></pre> <p>Note</p> <p>Unlike the BIDS specification, Neurobagel supports a <code>participants.tsv</code> file with a <code>session_id</code> field.</p>"},{"location":"dictionaries/#diagnosis","title":"Diagnosis","text":"<p>Terms from the SNOMED-CT ontology for clinical diagnosis. Terms from the National Cancer Institute Thesaurus for healthy control status.</p> <pre><code>{\n\"group\": {\n\"Description\": \"Group variable\",\n\"Levels\": {\n\"PD\": \"Parkinson's patient\",\n\"CTRL\": \"Control subject\",\n},\n\"Annotations\": {\n\"IsAbout\": {\n\"TermURL\": \"nb:Diagnosis\",\n\"Label\": \"Diagnosis\"\n},\n\"Levels\": {\n\"PD\": {\n\"TermURL\": \"snomed:49049000\",\n\"Label\": \"Parkinson's disease\"\n},\n\"CTRL\": {\n\"TermURL\": \"purl:NCIT_C94342\",\n\"Label\": \"Healthy Control\"\n}\n}\n}\n}\n}\n</code></pre> <p>The <code>IsAbout</code> relation uses a term from the Neurobagel namespace because <code>\"Diagnosis\"</code> is a standardized term.</p>"},{"location":"dictionaries/#sex","title":"Sex","text":"<p>Terms are from the SNOMED-CT ontology, which has controlled terms aligning with BIDS <code>participants.tsv</code> descriptions for sex.</p> <pre><code>{\n\"sex\": {\n\"Description\": \"Sex variable\",\n\"Levels\": {\n\"M\": \"Male\",\n\"F\": \"Female\"\n},\n\"Annotations\": {\n\"IsAbout\": {\n\"TermURL\": \"nb:Sex\",\n\"Label\": \"Sex\"\n},\n\"Levels\": {\n\"M\": {\n\"TermURL\": \"snomed:248153007\",\n\"Label\": \"Male\"\n},\n\"F\": {\n\"TermURL\": \"snomed:248152002\",\n\"Label\": \"Female\"\n}\n}\n}\n}\n}\n</code></pre> <p>The <code>IsAbout</code> relation uses a Neurobagel scoped term for <code>\"Sex\"</code> because  this is a Neurobagel common data element.</p>"},{"location":"dictionaries/#age","title":"Age","text":"<p>Neurobagel has a common data element for <code>\"Age\"</code> which describes a continuous column. To ensure age values are represented as floats in Neurobagel graphs, Neurobagel encodes the relevant \"heuristic\" describing the value format of a given age column. This heuristic, stored in the <code>Transformation</code> annotation, corresponds internally to a specific transformation that is used to convert the values to float ages.</p> <p>Possible heuristics: </p> TermURL Label nb:float float value nb:int integer value nb:euro european decimal value nb:bounded bounded value nb:iso8061 period of time defined according to the ISO8601 standard <pre><code>{\n\"age\": {\n\"Description\": \"Participant age\",\n\"Annotations\": {\n\"IsAbout\": {\n\"TermURL\": \"nb:Age\",\n\"Label\": \"Chronological age\"\n},\n\"Transformation\": {\n\"TermURL\": \"nb:euro\",\n\"Label\": \"European value decimals\"\n}\n}\n}\n}\n</code></pre>"},{"location":"dictionaries/#assessment-tool","title":"Assessment tool","text":"<p>For assessment tools like cognitive tests or rating scales,  Neurobagel encodes whether the tool was successfully completed. Because assessment tools often have several subscales or items  that can be stored as separate columns in the tabular <code>participant.tsv</code> file, each assessment tool column gets at least two annotations:</p> <ul> <li>one to classify it as <code>IsAbout</code> the generic category of assessment tools</li> <li>one to classify it as <code>PartOf</code> the specific assessment tool</li> </ul> <p>An additional annotation <code>MissingValues</code> can be used to specify value(s) in an assessment tool column which represent that the participant is missing a value/response for that subscale, when instances of missing values are present.</p> <pre><code>{\n\"updrs_1\": {\n\"Description\": \"item 1 scores for UPDRS\",\n\"Annotations\": {\n\"IsAbout\": {\n\"TermURL\": \"nb:Assessment\",\n\"Label\": \"Assessment tool\"\n},\n\"IsPartOf\": {\n\"TermURL\": \"cogatlas:tsk_4a57abb949ece\",\n\"Label\": \"Unified Parkinson's Disease Rating Scale\"\n}\n}\n},\n\"updrs_2\": {\n\"Description\": \"item 2 scores for UPDRS\",\n\"Annotations\": {\n\"IsAbout\": {\n\"TermURL\": \"nb:Assessment\",\n\"Label\": \"Assessment tool\"\n},\n\"IsPartOf\": {\n\"TermURL\": \"cogatlas:tsk_4a57abb949ece\",\n\"Label\": \"Unified Parkinson's Disease Rating Scale\"\n},\n\"MissingValues\": [\"\"]\n}\n}\n}\n</code></pre> <p>To determine whether a specific assessment tool is available for a given participant, we then combine all of the columns that were classified as <code>PartOf</code> that specific tool and then apply a simple <code>all()</code> heuristic to check that none of the columns contain any <code>MissingValues</code>.</p> <p>For the above example, this would be:</p> particpant_id updrs_1 updrs_2 sub-01 2 sub-02 1 1 <p>Therefore: </p> particpant_id updrs_available sub-01 False sub-02 True"},{"location":"term_naming_standards/","title":"Neurobagel standards for controlled term naming","text":""},{"location":"term_naming_standards/#naming-conventions","title":"Naming conventions","text":""},{"location":"term_naming_standards/#namespace-prefixes","title":"Namespace prefixes","text":"<ul> <li>Names should be all lowercase (e.g., <code>nidm</code>, <code>cogatlas</code>)</li> </ul>"},{"location":"term_naming_standards/#properties-graph-edges","title":"Properties (graph \"edges\")","text":"<ul> <li>Names should adhere to camelCase (uses capitalized words except for the first word/letter)</li> <li>Should be a compound of:<ul> <li>a verb relevant to the property (e.g., hasAge, isSubjectGroup)</li> <li>the range of the property, (e.g.,hasDiagnosis points to a Diagnosis object)</li> </ul> </li> </ul> <p>What this might look like in semantic triples: <pre><code>&lt;Subject&gt; &lt;nb:hasDiagnosis&gt; &lt;snomed:1234&gt;\n&lt;snomed:1234&gt; &lt;rdf:type&gt; &lt;nb:Diagnosis&gt;\n</code></pre></p>"},{"location":"term_naming_standards/#classes-or-resources-graph-nodes","title":"Classes or resources (graph \"nodes\")","text":"<ul> <li>Names should adhere to PascalCase (each word capitalized)</li> <li>Where possible, simplify to a single word (e.g., <code>Diagnosis</code>, <code>Dataset</code>, <code>Sex</code>)</li> </ul> <p>Note</p> <p>Generally, we own the terms for properties and classes (e.g., Diagnosis, Assessment) but not the resources representing instances of classes such as specific diagnosis, sex, or assessment values (these are reused from existing vocabularies).</p> <p>In cases where we reuse a term for a class that comes from an existing controlled vocabulary, and that vocabulary follows a different naming convention (e.g., all lowercase), we should follow the existing naming convention.</p>"},{"location":"term_naming_standards/#currently-used-namespaces","title":"Currently used namespaces","text":"Prefix IRI Types of terms <code>nb</code> http://neurobagel.org/vocab/ Neurobagel-\"owned\" properties and classes <code>nbg</code> http://neurobagel.org/graph/ Neurobagel graph databases <code>snomed</code> http://purl.bioontology.org/ontology/SNOMEDCT/ diagnosis and sex values <code>nidm</code> http://purl.org/nidash/nidm# imaging modalities <code>cogatlas</code> https://www.cognitiveatlas.org/task/id/ cognitive assessments and tasks"},{"location":"term_naming_standards/#what-if-an-nb-term-already-exists-in-another-controlled-vocabulary","title":"What if an <code>nb</code> term already exists in another controlled vocabulary?","text":"<p>If there is an equivalent controlled term to one we are defining in a different namespace,  we document this and express their equivalence using <code>owl:sameAs</code>.</p> <p>Example: If our term is <code>nb:Subject</code> and <code>nidm:Subject</code> is conceptually equivalent: <pre><code>&lt;nb:12345&gt; a &lt;nb:Subject&gt;\n&lt;nb:Subject&gt; a &lt;rdfs:Resource&gt;;\n    &lt;owl:sameAs&gt; &lt;nidm:Subject&gt;\n</code></pre></p>"},{"location":"term_naming_standards/#other-general-guidelines","title":"Other general guidelines","text":"<ul> <li>Each property (edge) should use a single namespace for the resources it corresponds to</li> <li>Where possible, hardcode or refer to identifiers and not human-readable labels</li> </ul>"},{"location":"mr_proc/code_org/","title":"Code organization","text":""},{"location":"mr_proc/code_org/#code-organization","title":"Code organization","text":"<p>mr_proc codebase is divided into data processing <code>workflows</code> and data availability <code>trackers</code>: </p> <ol> <li>workflow:<ul> <li>MRI data<ul> <li>Custom script to organize raw dicoms (i.e. scanner output) into a flat participant-level directory. </li> <li>Convert dicoms into BIDS using Heudiconv</li> <li>Runs a set of containerized MRI image processing pipelines </li> </ul> </li> <li>Tabular data<ul> <li>Custom scripts to organize raw tabular data (e.g. clinial assessments)</li> <li>Custom scripts to normalize and standardize data and metadata for downstream harmonization (see NeuroBagel)</li> </ul> </li> </ul> </li> <li>trackers:<ul> <li>Tracks available raw, standardized, and processed data</li> <li>Generates <code>bagels</code> for NeuorBagel graph and dashboard. </li> </ul> </li> </ol>"},{"location":"mr_proc/code_org/#legend","title":"legend","text":"<ul> <li>Red: dataset specific code and config</li> <li>Yellow: NeuroBagel interface</li> </ul>"},{"location":"mr_proc/configs/","title":"Configs","text":""},{"location":"mr_proc/configs/#global-files","title":"Global files","text":"<p>mr_proc consists of two global files for specifying local data and container paths and recruitment manifest </p>"},{"location":"mr_proc/configs/#global-configs-global_configsjson","title":"Global configs: <code>global_configs.json</code>","text":"<ul> <li>This is a dataset specific file and needs to be modified based on local configs and paths</li> <li>This file is used as an input to all workflow <code>run scripts</code> to read, process and track available data</li> <li>Copy, rename, and populate sample_global_configs.json </li> <li>This file contains:<ul> <li>Path to mr_proc_dataset</li> <li>List of pipelines + versions</li> <li>Path to local <code>container_store</code> comprising containers used by several workflow scripts</li> </ul> </li> </ul> <p>Suggestion</p> <p>Although not mandatory, for consistency the preferred location would be: <code>&lt;DATASET_ROOT&gt;/proc/global_configs.json</code>.</p>"},{"location":"mr_proc/configs/#sample-global_configsjson","title":"Sample <code>global_configs.json</code>","text":"<pre><code>{\n\"DATASET_NAME\": \"MyDataset\",\n\"DATASET_ROOT\": \"/path/to/MyDataset\",\n\"CONTAINER_STORE\": \"/path/to/container_store\",\n\"SINGULARITY_PATH\": \"singularity\",\n\"TEMPLATEFLOW_DIR\": \"/path/to/templateflow\",\n\n\"BIDS\": {\n\"heudiconv\": {\n\"VERSION\": \"0.11.6\",    \"CONTAINER\": \"heudiconv_{}.sif\",\n\"URL\": \"\"\n},\n\"validator\":{\n\"CONTAINER\": \"bids_validator.sif\",\n\"URL\": \"\"\n\n}\n},\n\n\"PROC_PIPELINES\": {\n\"mriqc\": {\n\"VERSION\": \"\",\n\"CONTAINER\": \"mriqc_{}.sif\",\n\"URL\": \"\"\n},\n\"fmriprep\": {\n\"VERSION\": \"20.2.7\",\n\"CONTAINER\": \"fmriprep_{}.sif\",\n\"URL\": \"\"\n},\n\"freesurfer\": {\n\"VERSION\": \"6.0.1\",\n\"CONTAINER\": \"fmriprep_{}.sif\",\n\"URL\": \"\"\n}\n}\n}\n</code></pre>"},{"location":"mr_proc/configs/#participant-manifest-mr_proc_manifestcsv","title":"Participant manifest: <code>mr_proc_manifest.csv</code>","text":"<ul> <li>This list serves as the ground-truth for subject and visit (i.e. session) availability</li> <li>Create the <code>mr_proc_manifest.csv</code> in <code>&lt;DATASET_ROOT&gt;/tabular/</code> comprising following columns<ul> <li><code>participant_id</code>: ID assigned during recruitment (at times used interchangably with subject_id)</li> <li><code>participant_dicom_dir</code>: participant-level dicom directory name on the disk</li> <li><code>visit</code>: label to denote participant visit for data acquisition (e.g. \"baseline\", \"m12\", \"m24\" or \"V01\", \"V02\" etc.)</li> <li><code>session</code>: alternative naming for visit - typically used for imaging data to comply with BIDS standard</li> <li><code>datatype</code>: a list of acquired imaging datatype as defined by BIDS standard</li> <li><code>bids_id</code>: this is created automatically which attaches <code>sub-</code> prefix and removes any non-alphanumeric chacaters (e.g. \"-\" or \"_\") from the original <code>participant_id</code> string. <code>participant_id</code> and <code>bids_id</code> in <code>mr_proc_manifest.csv</code> are used to link tabular and MRI data</li> </ul> </li> <li>New participant are appended upon recruitment as new rows</li> <li>Participants with multiple visits (i.e. sessions) should be added as separate rows</li> </ul>"},{"location":"mr_proc/configs/#sample-mr_proc_manifestcsv","title":"Sample <code>mr_proc_manifest.csv</code>","text":"participant_id participant_dicom_dir visit session datatype bids_id 001 MyStudy_001_2021 V01 ses-01 [\"anat\",\"dwi\",\"fmap\",\"func\"] sub-001 001 MyStudy_001_2022 V02 ses-02 [\"anat\"] sub-001 002 MyStudy_002_2021 V01 ses-01 [\"anat\",\"dwi\"] sub-002 002 MyStudy_002_2024 V03 ses-03 [\"anat\",\"dwi\"] sub-002"},{"location":"mr_proc/data_org/","title":"Data organization","text":""},{"location":"mr_proc/data_org/#data-organization","title":"Data organization","text":"<p>mr_proc dataset consists of a specific directory structure to organize MRI and tabular data</p> <p>Directories: </p> <ul> <li><code>tabular</code><ul> <li><code>demographics</code>: contains <code>mr_proc_manifest.csv</code></li> <li><code>assessments</code>: contains clinical assessments (e.g. MoCA) </li> </ul> </li> <li><code>downloads</code>: data dumps from remote data-stores (e.g. LONI)</li> <li><code>scratch</code>: space for un-organized data and wrangling</li> <li><code>dicom</code>: participant-level dicom dirs</li> <li><code>bids</code>: BIDS formatted dataset</li> <li><code>derivatives</code>: output of processing pipelines (e.g. fmriprep, mriqc)</li> <li><code>proc</code>: space for config and log files of the processing pipelines</li> <li><code>backups</code>: data backup space (tars)</li> <li><code>releases</code>: data releases (symlinks)</li> </ul> <p></p>"},{"location":"mr_proc/installation/","title":"Installation","text":""},{"location":"mr_proc/installation/#code-installation-and-dataset-setup","title":"Code installation and dataset setup","text":"<p>mr_proc workflow comprises mr_proc codebase that operates on mr_proc dataset with a specific directory structure, which is initialized with a tree.py script. </p>"},{"location":"mr_proc/installation/#mr_proc-codeenv-installation","title":"mr_proc code+env installation","text":"<ul> <li>Change dir to where you want to clone this repo, e.g.: <code>cd /home/&lt;user&gt;/projects/&lt;my_project&gt;/code/</code></li> <li>Create a new venv: <code>python3 -m venv mr_proc_env</code> </li> <li>Activate your env: <code>source mr_proc_env/bin/activate</code> </li> <li>Clone this repo: <code>git clone https://github.com/neurodatascience/mr_proc.git</code></li> <li>Change dir to <code>mr_proc</code> </li> <li>Install python dependencies: <code>pip install -e .</code> </li> </ul>"},{"location":"mr_proc/installation/#mr_proc-dataset-directory-setup","title":"mr_proc dataset directory setup","text":"<ul> <li>Run <code>python tree.py</code> to create mr_proc dataset directory tree</li> </ul> <p>Sample cmd: <pre><code>python tree.py --mr_proc_root &lt;DATASET_ROOT&gt;\n</code></pre></p> <ul> <li><code>data_disk</code>: data storage location on a local disk</li> <li><code>DATASET_ROOT</code>: root (starting point) of the mr_proc structured dataset</li> </ul> <p>Suggestion</p> <p>We suggest naming DATASET_ROOT directory after a study or a cohort. </p>"},{"location":"mr_proc/overview/","title":"Overview","text":""},{"location":"mr_proc/overview/#what-is-mr_proc","title":"What is mr_proc?","text":"<p>Process long and prosper</p> <p>mr_proc is a workflow manager for:</p> <ol> <li>MRI and tabular data curation</li> <li>Standardized processing </li> <li>Raw + processed data tracking</li> </ol>"},{"location":"mr_proc/overview/#modules","title":"Modules","text":"<ol> <li><code>Code</code>: Codebase repo for running and tracking workflows</li> <li><code>Data</code>: Dataset organized in a specific directory structure</li> <li><code>Containers</code>: Singularity containers encapsulating your processing pipelines</li> </ol>"},{"location":"mr_proc/overview/#objectives","title":"Objectives","text":"<ol> <li>Standardized data i.e. convert DICOMs into BIDS</li> <li>Run commonly used image processing pipelines e.g. FreeSurfer, fMRIPrep</li> <li>Organize processed MR data inside <code>derivatives</code> directory</li> <li>Organize demographic and clinical assessment data inside <code>tabular</code> directory</li> <li>Run tracker scripts to populate <code>bagel.csv</code> with tabular and processing pipeline metadata</li> <li>Provide metadata to <code>NeuroBagel</code> to allow dashboarding and querying participants across multiple studies</li> </ol>"},{"location":"mr_proc/overview/#organization","title":"Organization","text":"<p>Organization of <code>Code</code>, <code>Data</code>, and <code>Container</code> modules</p> <p></p>"},{"location":"mr_proc/overview/#steps","title":"Steps","text":"<p>The mr_proc workflow steps and linked identifiers (i.e. participant_id, dicom_id, bids_id) are show below:</p> <p></p>"},{"location":"mr_proc/workflow/bids_conv/","title":"BIDS conversion","text":""},{"location":"mr_proc/workflow/bids_conv/#objective","title":"Objective","text":"<p>Convert DICOMs to BIDS using Heudiconv (tutorial). </p>"},{"location":"mr_proc/workflow/bids_conv/#key-directories-and-files","title":"Key directories and files","text":"<ul> <li><code>&lt;DATASET_ROOT&gt;/dicom</code></li> <li><code>&lt;DATASET_ROOT&gt;/bids</code></li> <li><code>heuristic.py</code></li> </ul>"},{"location":"mr_proc/workflow/bids_conv/#procedure","title":"Procedure","text":"<ul> <li>Ensure you have the appropriate HeuDiConv container listed in your <code>global_configs.json</code></li> <li>Use run_bids_conv.py to run HeuDiConv <code>stage_1</code> and <code>stage_2</code>.  </li> <li>Run <code>stage_1</code> to generate a list of available protocols from the DICOM header. These protocols are listed in <code>&lt;DATASET_ROOT&gt;/bids/.heudiconv/&lt;participant_id&gt;/info/dicominfo_ses-&lt;session_id&gt;.tsv</code></li> </ul> <p>Sample cmd: <pre><code>python run_bids_conv.py \\\n--global_config &lt;global_config_file&gt; \\\n--session_id &lt;session_id&gt; \\\n--stage 1\n</code></pre></p> <p>Note</p> <p>If participants have multiple sessions (or visits), these need to be converted separately and combined post-hoc to avoid Heudiconv errors. </p> <ul> <li>Copy+Rename sample_heuristic.py to <code>heuristic.py</code> in the code repo itself. Then edit <code>./heuristic.py</code> to create a name-mapping (i.e. dictionary) for BIDS organization based on the list of available protocols. </li> </ul> <p>Note</p> <p>This file automatically gets copied into <code>&lt;DATASET_ROOT&gt;/proc/heuristic.py</code> to be seen by the Singularity container.</p> <ul> <li>Run <code>stage_2</code> to convert the dicoms into BIDS format based on the mapping from <code>heuristic.py</code>. </li> </ul> <p>Sample cmd: <pre><code>python run_bids_conv.py \\\n   --global_config &lt;global_config_file&gt; \\\n   --session_id &lt;session_id&gt; \\\n   --stage 2\n</code></pre></p> <p>Note</p> <p>Once <code>heuristic.py</code> is finalized, only <code>stage_2</code> needs to be run peridodically unless new scan protocol is added.</p>"},{"location":"mr_proc/workflow/bids_conv/#bids-validator","title":"BIDS validator","text":"<ul> <li>Make sure you have the appropriate bids_validator container in your <code>&lt;DATASET_ROOT&gt;/proc/global_configs.json</code></li> <li>Use run_bids_val.sh to check for errors and warnings<ul> <li>Sample command: <code>run_bids_val.sh &lt;bids_dir&gt; &lt;log_dir&gt;</code> </li> <li>Alternatively if your machine has a browser you can also use an online validator</li> </ul> </li> </ul> <p>Note</p> <p>Make sure you match the version of Heudiconv and BIDS validator standard</p> <p>Note</p> <p>Heuristic file needs to be updated if your dataset has different protocols for different participants</p>"},{"location":"mr_proc/workflow/bids_conv/#fix-heudiconv-errors","title":"Fix HeuDiConv errors","text":"<ul> <li>If you see errors from BIDS validator, it is possible that HeuDiConv may not be supporting your MRI sequence. In that case add a function to fix_heudiconv_issues.py to manually rename files, and run the script posthoc. </li> <li>Make sure to open an issue on HeuDiConv Github for fix in future release. </li> </ul>"},{"location":"mr_proc/workflow/dicom_org/","title":"DICOM organization","text":""},{"location":"mr_proc/workflow/dicom_org/#objective","title":"Objective","text":"<p>This is a dataset specific process and needs to be customized based on local scanner DICOM dumps and file naming. This organization should produce, for a given session, participant specific dicom dirs. Each of these participant-dir contains a flat list of dicoms for the participant for all available imaging modalities and scan protocols.</p>"},{"location":"mr_proc/workflow/dicom_org/#key-directories-and-files","title":"Key directories and files","text":"<ul> <li><code>&lt;DATASET_ROOT&gt;/downloads</code></li> <li><code>&lt;DATASET_ROOT&gt;/scratch/raw_dicoms</code></li> <li><code>&lt;DATASET_ROOT&gt;/dicom</code></li> <li><code>&lt;DATASET_ROOT&gt;/tabular/demographics/mr_proc_manifest.csv</code></li> </ul>"},{"location":"mr_proc/workflow/dicom_org/#procedure","title":"Procedure","text":"<ul> <li>Download DICOM dumps (e.g. ZIPs / tarballs) in the <code>&lt;DATASET_ROOT&gt;/downloads</code> directory. It is recommended that different visits (i.e. sessions) are downloaded in separate sub-directories and named as listed in the <code>global_config.json</code>.</li> <li>Extract (and rename* if needed) all participants into <code>&lt;DATASET_ROOT&gt;/scratch/raw_dicoms</code> separately for each visit (i.e. session). </li> </ul> <p>Note</p> <p>IMPORTANT: the participant-level directory names should match participant_ids in the mr_proc_manifest.csv. It is recommended to use participant_id naming format to exclude any non-alphanumeric chacaters (e.g. \"-\" or \"_\"). If your participant_id does contain these characters, it is still recommended to remove them from the participant-level DICOM directory names (e.g., QPN_001 --&gt; QPN001).  </p> <p>Note</p> <p>It is okay for the participant directory to have messy internal subdir tree with DICOMs from multiple modalities. (See data org schematic for details). The run script will search and validate all available DICOM files automatically. </p> <ul> <li>Run run_dicom_org.py to:<ul> <li>Search: Find all the DICOMs inside the participant directory. </li> <li>Validate: Excludes certain individual dicom files that are invalid or contain scanner-derived data not compatible with BIDS conversion.</li> <li>Copy or Symlink (Recommended): Creates symlinks from <code>raw_dicoms/</code> to the <code>&lt;DATASET_ROOT&gt;/dicom</code>, where all participant specific dicoms are in a flat list.</li> </ul> </li> </ul> <p>Sample cmd: <pre><code>python run_dicom_org.py \\\n    --global_config &lt;global_config_file&gt; \\\n    --session_id &lt;session_id&gt; \\\n    --use_symlinks \n</code></pre></p>"},{"location":"mr_proc/workflow/proc_pipe/fmriprep/","title":"fmriprep","text":""},{"location":"mr_proc/workflow/proc_pipe/fmriprep/#objective","title":"Objective","text":"<p>Run fMRIPrep pipeline on BIDS formatted dataset. Note that a standard fMRIPrep run also include FreeSurfer processing.</p>"},{"location":"mr_proc/workflow/proc_pipe/fmriprep/#key-directories-and-files","title":"Key directories and files","text":"<ul> <li><code>&lt;DATASET_ROOT&gt;/bids</code></li> <li><code>&lt;DATASET_ROOT&gt;/derivatives/fmriprep/</code></li> <li><code>&lt;DATASET_ROOT&gt;/derivatives/freesurfer/</code></li> <li><code>bids_filter.json</code></li> </ul>"},{"location":"mr_proc/workflow/proc_pipe/fmriprep/#procedure","title":"Procedure","text":"<ul> <li>Ensure you have the appropriate fMRIPrep container listed in your <code>global_configs.json</code> </li> <li>Use run_fmriprep.py script to run fmriprep pipeline. </li> <li>You can run \"anatomical only\" workflow by adding <code>--anat_only</code> flag</li> <li>(Optional) Copy+Rename sample_bids_filter.json to <code>bids_filter.json</code> in the code repo itself. Then edit <code>bids_filter.json</code> to filter certain modalities / acquisitions. This is common when you have multiple T1w acquisitions (e.g. Neuromelanin, SPIR etc.) for a given modality. </li> </ul> <p>Note</p> <p>When <code>--use_bids_filter</code> flag is set, this <code>bids_filter.json</code> is automatically copied into <code>&lt;DATASET_ROOT&gt;/bids/bids_filter.json</code> to be seen by the Singularity container.</p> <ul> <li>For FreeSurfer tasks, you need to have a license.txt file inside <code>&lt;DATASET_ROOT&gt;/derivatives/freesurfer/</code></li> <li>fMRIPrep manages brain-template spaces using TemplateFlow. These templates can be shared across studies and datasets. Use <code>global_configs.json</code> to specify path to <code>TEMPLATEFLOW_DIR</code> where these templates can reside. </li> </ul> <p>Note</p> <p>For machines with Internet connections, all required templates are automatically downloaded duing the fMRIPrep run.</p> <p>Sample cmd: <pre><code>python run_fmriprep.py \\\n--global_config &lt;global_config_file&gt; \\\n--participant_id MNI01 \\\n--session_id 01 \\\n--use_bids_filter </code></pre></p> <p>Note</p> <p>Unlike DICOM and BIDS run scripts, <code>run_fmriprep.py</code> can only process 1 participant at a time due to heavy compute requirements of fMRIPrep. For parallel processing on a cluster, sample HPC job scripts (slurm and sge) are provided in hpc subdir. </p> <p>Note</p> <p>You can change default run parameters in the run_fmriprep.sh by looking at the documentation</p> <p>Note</p> <p>Clean-up working dir (<code>fmriprep_wf</code>): fMRIPrep run generates huge number of intermediate files. You should remove those after successful run to free-up space.</p>"},{"location":"mr_proc/workflow/proc_pipe/fmriprep/#fmriprep-tasks","title":"fMRIPrep tasks","text":"<ul> <li>Main MR processing tasks run by fmriprep (see fMRIPrep for details):<ul> <li>Preprocessing<ul> <li>Bias correction / Intensity normalization (N4)</li> <li>Brain extraction (ANTs)</li> <li>Spatial normalization to standard space(s)</li> </ul> </li> <li>Anatomical<ul> <li>Tissue segmentation (FAST)</li> <li>FreeSurfer recon-all</li> </ul> </li> <li>Functional<ul> <li>BOLD reference image estimation</li> <li>Head-motion estimation</li> <li>Slice time correction</li> <li>Susceptibility Distortion Correction (SDC)</li> <li>Pre-processed BOLD in native space</li> <li>EPI to T1w registration</li> <li>Resampling BOLD runs onto standard spaces</li> <li>EPI sampled to FreeSurfer surfaces</li> <li>Confounds estimation</li> <li>ICA-AROMA (not run by default)</li> </ul> </li> <li>Qualtiy Control<ul> <li>Visual reports</li> </ul> </li> </ul> </li> </ul>"}]}